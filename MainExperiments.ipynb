{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Reconstruction Methods for recovering the degree distribution \n",
    "\n",
    "This notebook is implementing some of Raul's methods for recovering the degree distribution of edge-sampled networks. \n",
    "\n",
    "## Notation:\n",
    "* $G(V,E)$ original graph with $N$ nodes, $M$ links. \n",
    "* Sampled graph $G'(V',E')$ with $N'$ nodes, $M'$ links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sampling_utils import *\n",
    "from plotting_utils import *\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "a4_dims = (11.69,8.27)\n",
    "\n",
    "from scipy.stats import rv_discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MME Estimator\n",
    "\n",
    "The first estimator we use is what was described in Ganguly et al's paper as the MME (method of moments estimator), defined as $\\hat{k} = k'/p$. It is an unbiased maximum likelihood estimator for the parameter $k$ in a binomial random variable $k' \\sim B(k,p)$.\n",
    "\n",
    "### Notes \n",
    "* We round each obtained degree to an integer.\n",
    "* Because of observed sampled degrees being an integer, all estimated degrees are multiples of $1/p$. E.g. if $p=0.05$ our estimated degrees will all be something like 20, 40 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  4 10  4]\n"
     ]
    }
   ],
   "source": [
    "## Basic estimator for degree sequence which does not redistribute any spare or missing links\n",
    "def deg_MME_basic(k_seq,sample_prob,as_int=False):\n",
    "    if sample_prob==0.0:\n",
    "        return np.zeros(shape=len(k_seq), dtype=float)\n",
    "    estimated = k_seq/sample_prob\n",
    "    if (as_int):\n",
    "        return np.array([*map(int,estimated)])\n",
    "    return estimated\n",
    "\n",
    "print(deg_MME_basic(np.array([3,2,5,2]),0.5,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected MME estimator\n",
    "\n",
    "Here we take into account the estimated number of links $M'/p$. Depending on the value of $p$ (specifically, whether or not $1/p$ is an integer), since each degree is rounded, we may not have the relation $\\lfloor 2M'/p \\rfloor = \\sum_{i=1}^{N'} \\lfloor k_i'/p \\rfloor $ that is an identity for proper graph degree sequences. We may have too high or too low *total degree* (RHS).\n",
    "\n",
    "In this procedure, we first calculate the basic MME estimator for the degree sequence and for the number of links, then adjust for differences by either randomly adding links or randomly removing links as required.\n",
    "\n",
    "### Notes\n",
    "* If $1/p$ is an integer, this procedure won't change anything as the estimated number of links (multiplied by 2) will be equal to the expected degree sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimator which first obtains MME one then deals with rounding discrepency by redistributing links\n",
    "def deg_MME(kp_seq, probability_keep, redistribute_links=True):\n",
    "\n",
    "    # Number of nodes/links in sampled graph\n",
    "    N_prime, M_prime = len(kp_seq), sum(kp_seq)/2\n",
    "\n",
    "    ## Estimated number of links from sampled graph\n",
    "    M_estimated = int(M_prime/probability_keep)\n",
    "\n",
    "    ## Sampled and scaled-up degree distribution\n",
    "    k_prime = np.array(kp_seq)\n",
    "    k_est = deg_MME_basic(k_seq=k_prime,sample_prob=probability_keep,as_int=True)\n",
    "\n",
    "    if (redistribute_links==False):\n",
    "        return np.array(k_est)\n",
    "\n",
    "    ## \"Left over\" stubs from the rounding process\n",
    "    k_spare = 2*M_estimated - sum(k_est)\n",
    "\n",
    "    ## Distribute these random stubs if there are any\n",
    "    if k_spare>0:\n",
    "        sampled_nodes = random.sample(range(N_prime),k_spare)\n",
    "        for node in sampled_nodes:\n",
    "            k_est[node]+=1\n",
    "\n",
    "    ## If we have given nodes more connections than there are links, randomly remove some\n",
    "    if k_spare<0:\n",
    "        non_isolated_nodes = list(filter(lambda ind: k_est[ind]>0, range(N_prime)))\n",
    "        sampled_nodes = random.sample(non_isolated_nodes,abs(k_spare))\n",
    "        for node in sampled_nodes:\n",
    "            k_est[node]-=1\n",
    "    \n",
    "    return np.array(k_est) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Risk Minimiser\n",
    "\n",
    "from Estimation of Vertex Degrees in a Sampled Network, A. Ganguly and E. Kolaczyk. Does not improve upon the MME very much at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_RME(kp_seq, probability_keep):\n",
    "    k_prime = np.array(kp_seq)\n",
    "\n",
    "    minimiser = lambda k: k**2/(probability_keep * (k + 1 - probability_keep)) + (1-probability_keep)/probability_keep\n",
    "    return minimiser(k_prime)\n",
    "\n",
    "# H = nx.erdos_renyi_graph(10,0.1)\n",
    "# print(deg_RME(H,0.5))\n",
    "# print(deg_MME(H,0.5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo estimator\n",
    "\n",
    "This process starts off with the degree sequence estimated by the MME and tries to improve it by comparing samples from that estimated degree sequence of $G$ with our observed degree sequence in $G'$. \n",
    "\n",
    "At each step, a link in the estimated degree distribution is randomly rewired and we calculate the expected degree sequence if we were link-sampling it with probability $p$. If it is closer to the degree sequence of $G'$ than before, then we accept the proposed rewire, otherwise reject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process for randomly redistributing links\n",
    "def monte_carlo_degree(kp_seq,probability_keep):\n",
    "\n",
    "    # Number of nodes/links in sampled graph\n",
    "    N_prime, M_prime = len(kp_seq), sum(kp_seq)/2\n",
    "\n",
    "    observed_degree = np.array(kp_seq)\n",
    "    estimated_degree = deg_MME(kp_seq, probability_keep=probability_keep,redistribute_links=True)\n",
    "\n",
    "    ## Expected degree of sampled network according to binomial\n",
    "    sampled_sequence = estimated_degree * probability_keep *1.0\n",
    "    #print(sampled_sequence)\n",
    "\n",
    "    ## Sum of squared distances as base quality metric\n",
    "    ssd_current = sum((sampled_sequence - observed_degree)**2)\n",
    "    #print(observed_degree)\n",
    "    \n",
    "    # Commence Monte Carlo process\n",
    "    # NB stuck in global minima if 1/p is an integer.\n",
    "    cts_accept=0\n",
    "    for i in range(15000):\n",
    "        valid_indices = [ind for ind in range(N_prime) if estimated_degree[ind]>1]\n",
    "\n",
    "        ## Randomly rewire an edge\n",
    "        [n1, n2] = random.sample(valid_indices,2)\n",
    "        # if i==1:\n",
    "        #     print(n1,n2)\n",
    "\n",
    "        # ## Ensure we don't leave any isolated nodes\n",
    "        # while(estimated_degree[n1]<=1):\n",
    "        #     [n1,n2] = random.sample(range(N_prime),2)\n",
    "        # estimated_degree[n1]-=1\n",
    "        # estimated_degree[n2]+=1\n",
    "\n",
    "        sampled_sequence = estimated_degree * probability_keep\n",
    "        ssd_new = sum((sampled_sequence - observed_degree)**2)\n",
    "\n",
    "        ## Reject step if error is larger:\n",
    "        if (ssd_new > ssd_current):\n",
    "            estimated_degree[n1]+=1\n",
    "            estimated_degree[n2]-=1\n",
    "        else:\n",
    "            ssd_current = ssd_new\n",
    "            cts_accept+=1\n",
    "\n",
    "    return estimated_degree, ssd_current, cts_accept\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link cascade method\n",
    "Previously used methods do not take into account isolated nodes, they only compare the difference in degree for nodes that are in $G'$ (i.e. those that end up with degree $>=1$). The following method assumes we know the number of nodes which end up being degree 0 in $G'$.\n",
    "\n",
    "1. Start off with the MME estimate of the degree sequence (including the zeros for isolated nodes).\n",
    "2. Rank the nodes in decreasing order of degree.\n",
    "3. Find the highest rank node that has degree 0, and steal a link from the node directly above it in the rankings.\n",
    "4. Repeat step 3 until there are no zero-degree nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_cascade(kp_seq, no_isolates, probability_keep):\n",
    "    \n",
    "    # Number of nodes/links in sampled graph\n",
    "    N_prime, M_prime = len(kp_seq), sum(kp_seq)/2\n",
    "\n",
    "    ## Run monte carlo \n",
    "    mme_degrees_wo_isolates = deg_MME(kp_seq=kp_seq,probability_keep=probability_keep,redistribute_links=True)\n",
    "    mme_degrees = np.concatenate([mme_degrees_wo_isolates,np.zeros(no_isolates,dtype=int)])\n",
    "\n",
    "    ## get the permutation of degrees into right order and the reverse of this.\n",
    "    node_index = np.argsort(-1*mme_degrees)\n",
    "    reverse_index = np.argsort(node_index)\n",
    "\n",
    "    inds = list(filter(lambda i: mme_degrees[i]==0,node_index))\n",
    "\n",
    "    ## make sorted list as a copy\n",
    "    mme_degrees_copy = [mme_degrees[i] for i in node_index]\n",
    "    while (len(inds)>0):\n",
    "        # find highest node in the list that has degree 0.\n",
    "        ind = inds.pop(0)\n",
    "        mme_degrees_copy[ind-1] -= 1\n",
    "        mme_degrees_copy[ind] += 1\n",
    "        if (mme_degrees_copy[ind-1]==0):\n",
    "            inds.insert(0,ind-1)\n",
    "\n",
    "    cascaded_degrees = [mme_degrees_copy[i] for i in reverse_index]\n",
    "    return cascaded_degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes method\n",
    "\n",
    "This part uses the Bayes method of chapter 6 of my thesis to construct a posterior degree distribution. However, this time, instead of just using Poisson and true prior, we use these 3 estimates of the degree distribution as a prior, named \"MME\", \"Cascaded\" and \"Monte Carlo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bayes estimate, nb supply the prior as a degree sequence rather than counts of different degrees.\n",
    "def bayes_approx(prior_as_sequence,sampled_degrees,prob_retain):\n",
    "    observed = np.array(sampled_degrees)\n",
    "    N_approx = len(prior_as_sequence)\n",
    "\n",
    "    ## transform from degree sequence to degree distribution\n",
    "    deg_counts = Counter(prior_as_sequence)\n",
    "\n",
    "    ## max value to use for the degree -- pick kmax as the largest support value of the prior.\n",
    "    k_max = max(deg_counts.elements())\n",
    "\n",
    "    ## construct prior from approx sequence\n",
    "    prior = [deg_counts[k]/N_approx for k in range(k_max+1)]\n",
    "    posterior = np.zeros(N_approx)\n",
    "\n",
    "    for i in range(N_approx):\n",
    "        k_observed = observed[i]\n",
    "        k_range = np.array(range(k_observed,k_max+1))\n",
    "\n",
    "        ## lambda function so can be applied to numpy array\n",
    "        binom_ev = lambda k : binom(k,k_observed,prob_retain)\n",
    "\n",
    "        denom = np.dot(binom_ev(k_range),prior[k_observed:k_max+1])\n",
    "        numer = np.dot(binom_ev(k_range)*k_range,prior[k_observed:k_max+1])\n",
    "        if (denom>0):\n",
    "            posterior[i]=numer/denom\n",
    "    return posterior\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility function for mean squared difference between two arrays\n",
    "def mse(seq1, seq2):\n",
    "    return sum((seq1-seq2)**2)/len(seq1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main experiment. \n",
    "\n",
    "We start with an Erdos-Renyi graph of 1000 nodes and 5000 edges (average degree 10). We make edge samples going from $p=0.05$ to $p=1.0$ going up by 0.05. For each parameter we run 10 experiments and collect the mean and sd to get error bars.\n",
    "\n",
    "The estimators we are interested in are:\n",
    "* MME ($k'/p$ estimator)\n",
    "* \"Cascaded\" links estimator\n",
    "* Monte Carlo estimator\n",
    "* Bayes with MME prior\n",
    "* Bayes with cascaded links prior\n",
    "* Bayes with Monte Carlo prior\n",
    "* Bayes with true prior (we expect this to give an almost exact answer)\n",
    "\n",
    "The error metric to compare the real and estimated degree sequence is the mean squared error in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 1\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 2\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 3\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 4\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 5\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 6\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 7\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 8\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Experiment 9\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n",
      "Monte Carlo Degree\n",
      "Bayes\n"
     ]
    }
   ],
   "source": [
    "folder = \"STACKEX\"\n",
    "net_name = \"SX\"\n",
    "p_range = np.linspace(0.1,0.9,9)\n",
    "\n",
    "G = nx.read_edgelist(folder+\"/REAL\")\n",
    "\n",
    "experiments = 10\n",
    "\n",
    "mme = np.zeros((experiments,len(p_range)))\n",
    "monte_carlo = np.zeros((experiments,len(p_range)))\n",
    "cascade = np.zeros((experiments,len(p_range)))\n",
    "bayes_mc = np.zeros((experiments,len(p_range)))\n",
    "bayes_csc = np.zeros((experiments,len(p_range)))\n",
    "bayes_true = np.zeros((experiments,len(p_range)))\n",
    "\n",
    "def single_run(run_number):\n",
    "    for ind, p in enumerate(p_range):\n",
    "        H = nx.read_edgelist(folder+\"/\"+net_name+str(round(p,1))+\"-\"+str(run_number))\n",
    "        kp_seq = [d for (_,d) in nx.degree(H)]\n",
    "\n",
    "        ## True degree sequence of nodes in G that are in H\n",
    "        true_degrees = np.array([nx.degree(G)[n] for n in H.nodes])\n",
    "        kp_full = kp_seq+[0 for _ in range(G.number_of_nodes() - H.number_of_nodes())]\n",
    "        full_degrees = np.array([nx.degree(G)[n] for n in H.nodes]+[nx.degree(G)[n] for n in G.nodes if n not in H.nodes])\n",
    "\n",
    "        ## MME and associated error\n",
    "        # print(\"Calculating MME\")\n",
    "        #start = time.time()\n",
    "\n",
    "        mme_seq = deg_MME(kp_seq,p,True)\n",
    "        mme[run_number,ind] = mse(mme_seq,true_degrees)\n",
    "        # print(time.time() - start)\n",
    "\n",
    "        # Cascaded degrees and error\n",
    "        # print(\"Cascaded degrees\")\n",
    "        # start = time.time()\n",
    "\n",
    "        csc_seq = degree_cascade(kp_seq,len(full_degrees) - len(true_degrees),p)\n",
    "        cascade[run_number,ind] = mse(csc_seq,full_degrees)\n",
    "        #print(time.time() - start)\n",
    "\n",
    "        ## Monte carlo improvement\n",
    "        print(\"Monte Carlo Degree\")\n",
    "        # start = time.time()\n",
    "\n",
    "        deg, ssd_error, accepted = monte_carlo_degree(kp_seq,p)\n",
    "        monte_carlo[run_number,ind] = mse(deg,true_degrees)\n",
    "        # print(time.time() - start)\n",
    "        \n",
    "        # Bayes using monte carlo\n",
    "        print(\"Bayes\")\n",
    "        # start = time.time()\n",
    "\n",
    "        posterior_mc = bayes_approx(deg,kp_seq,p)\n",
    "        bayes_mc[run_number,ind] = mse(posterior_mc,true_degrees)\n",
    "        #print(time.time() - start)\n",
    "\n",
    "        # Bayes using cascaded degrees\n",
    "        # print(\"Bayes Cascaded\")\n",
    "        # start = time.time()\n",
    "\n",
    "        csc_H = np.array(csc_seq[:H.number_of_nodes()])\n",
    "        csc_bayes_degrees = bayes_approx(csc_H,kp_seq,p)\n",
    "        bayes_csc[run_number,ind] = mse(csc_bayes_degrees,true_degrees)\n",
    "        # print(time.time() - start)\n",
    "        \n",
    "        # Bayes using true prior\n",
    "        # print(\"Bayes True Prior\")\n",
    "        # start = time.time()     \n",
    "\n",
    "        posterior_true = bayes_approx(true_degrees,kp_seq,p)\n",
    "        bayes_true[run_number,ind] = mse(posterior_true,true_degrees)\n",
    "        #print(time.time() - start)\n",
    "\n",
    "for ex in range(experiments):\n",
    "    print(\"Experiment \"+str(ex))\n",
    "    single_run(ex)\n",
    "    \n",
    "mme_means, mme_sds = mme.mean(axis=0), mme.std(axis=0)\n",
    "csc_means, csc_sds = cascade.mean(axis=0), cascade.std(axis=0)\n",
    "monte_carlo_means, monte_carlo_sds = monte_carlo.mean(axis=0), monte_carlo.std(axis=0)\n",
    "bayes_mc_means, bayes_mc_sds = bayes_mc.mean(axis=0), bayes_mc.std(axis=0)\n",
    "bayes_mc_means, bayes_mc_sds = bayes_mc.mean(axis=0), bayes_mc.std(axis=0)\n",
    "bayes_csc_means, bayes_csc_sds = bayes_csc.mean(axis=0), bayes_csc.std(axis=0)\n",
    "bayes_true_means, bayes_true_sds = bayes_true.mean(axis=0), bayes_true.std(axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mme_mean</th>\n",
       "      <th>mme_sd</th>\n",
       "      <th>csc_mean</th>\n",
       "      <th>csc_sd</th>\n",
       "      <th>monte_carlo_mean</th>\n",
       "      <th>monte_carlo_sd</th>\n",
       "      <th>bayes_mc_mean</th>\n",
       "      <th>bayes_mc_sd</th>\n",
       "      <th>bayes_csc_mean</th>\n",
       "      <th>bayes_csc_sd</th>\n",
       "      <th>bayes_true_mean</th>\n",
       "      <th>bayes_true_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>333.933686</td>\n",
       "      <td>14.623881</td>\n",
       "      <td>149.785533</td>\n",
       "      <td>6.339386</td>\n",
       "      <td>333.933686</td>\n",
       "      <td>14.623881</td>\n",
       "      <td>334.827368</td>\n",
       "      <td>17.976412</td>\n",
       "      <td>334.823655</td>\n",
       "      <td>17.976096</td>\n",
       "      <td>310.815833</td>\n",
       "      <td>17.173429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>105.629258</td>\n",
       "      <td>4.319701</td>\n",
       "      <td>66.549529</td>\n",
       "      <td>2.213887</td>\n",
       "      <td>105.629258</td>\n",
       "      <td>4.319701</td>\n",
       "      <td>105.177664</td>\n",
       "      <td>4.385885</td>\n",
       "      <td>105.176894</td>\n",
       "      <td>4.385886</td>\n",
       "      <td>99.719351</td>\n",
       "      <td>4.684317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>49.385915</td>\n",
       "      <td>2.820288</td>\n",
       "      <td>34.791987</td>\n",
       "      <td>1.857534</td>\n",
       "      <td>49.404771</td>\n",
       "      <td>2.820185</td>\n",
       "      <td>49.365780</td>\n",
       "      <td>2.827551</td>\n",
       "      <td>49.359822</td>\n",
       "      <td>2.826703</td>\n",
       "      <td>46.827814</td>\n",
       "      <td>2.812289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>29.144715</td>\n",
       "      <td>0.840102</td>\n",
       "      <td>22.999394</td>\n",
       "      <td>0.603890</td>\n",
       "      <td>29.173236</td>\n",
       "      <td>0.868726</td>\n",
       "      <td>28.756119</td>\n",
       "      <td>0.838195</td>\n",
       "      <td>28.751993</td>\n",
       "      <td>0.839632</td>\n",
       "      <td>27.593595</td>\n",
       "      <td>0.887082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>17.776540</td>\n",
       "      <td>0.594388</td>\n",
       "      <td>15.199087</td>\n",
       "      <td>0.477177</td>\n",
       "      <td>17.776540</td>\n",
       "      <td>0.594388</td>\n",
       "      <td>17.896803</td>\n",
       "      <td>0.519504</td>\n",
       "      <td>17.896760</td>\n",
       "      <td>0.519504</td>\n",
       "      <td>17.170802</td>\n",
       "      <td>0.556397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>11.714102</td>\n",
       "      <td>0.456601</td>\n",
       "      <td>12.563617</td>\n",
       "      <td>0.468586</td>\n",
       "      <td>11.709663</td>\n",
       "      <td>0.447574</td>\n",
       "      <td>11.384576</td>\n",
       "      <td>0.439079</td>\n",
       "      <td>11.382669</td>\n",
       "      <td>0.438819</td>\n",
       "      <td>10.874407</td>\n",
       "      <td>0.392665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>7.090026</td>\n",
       "      <td>0.231785</td>\n",
       "      <td>7.401842</td>\n",
       "      <td>0.339521</td>\n",
       "      <td>7.092579</td>\n",
       "      <td>0.237912</td>\n",
       "      <td>6.897615</td>\n",
       "      <td>0.232525</td>\n",
       "      <td>6.899665</td>\n",
       "      <td>0.229161</td>\n",
       "      <td>6.579686</td>\n",
       "      <td>0.267581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>4.238758</td>\n",
       "      <td>0.140996</td>\n",
       "      <td>4.134004</td>\n",
       "      <td>0.119655</td>\n",
       "      <td>4.238542</td>\n",
       "      <td>0.142697</td>\n",
       "      <td>4.052979</td>\n",
       "      <td>0.129510</td>\n",
       "      <td>4.052448</td>\n",
       "      <td>0.128638</td>\n",
       "      <td>3.790642</td>\n",
       "      <td>0.093496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>1.948392</td>\n",
       "      <td>0.058093</td>\n",
       "      <td>2.213038</td>\n",
       "      <td>0.069015</td>\n",
       "      <td>1.955892</td>\n",
       "      <td>0.056969</td>\n",
       "      <td>1.738521</td>\n",
       "      <td>0.067539</td>\n",
       "      <td>1.736631</td>\n",
       "      <td>0.067456</td>\n",
       "      <td>1.591423</td>\n",
       "      <td>0.063267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mme_mean     mme_sd    csc_mean    csc_sd  monte_carlo_mean  \\\n",
       "0.1  333.933686  14.623881  149.785533  6.339386        333.933686   \n",
       "0.2  105.629258   4.319701   66.549529  2.213887        105.629258   \n",
       "0.3   49.385915   2.820288   34.791987  1.857534         49.404771   \n",
       "0.4   29.144715   0.840102   22.999394  0.603890         29.173236   \n",
       "0.5   17.776540   0.594388   15.199087  0.477177         17.776540   \n",
       "0.6   11.714102   0.456601   12.563617  0.468586         11.709663   \n",
       "0.7    7.090026   0.231785    7.401842  0.339521          7.092579   \n",
       "0.8    4.238758   0.140996    4.134004  0.119655          4.238542   \n",
       "0.9    1.948392   0.058093    2.213038  0.069015          1.955892   \n",
       "\n",
       "     monte_carlo_sd  bayes_mc_mean  bayes_mc_sd  bayes_csc_mean  bayes_csc_sd  \\\n",
       "0.1       14.623881     334.827368    17.976412      334.823655     17.976096   \n",
       "0.2        4.319701     105.177664     4.385885      105.176894      4.385886   \n",
       "0.3        2.820185      49.365780     2.827551       49.359822      2.826703   \n",
       "0.4        0.868726      28.756119     0.838195       28.751993      0.839632   \n",
       "0.5        0.594388      17.896803     0.519504       17.896760      0.519504   \n",
       "0.6        0.447574      11.384576     0.439079       11.382669      0.438819   \n",
       "0.7        0.237912       6.897615     0.232525        6.899665      0.229161   \n",
       "0.8        0.142697       4.052979     0.129510        4.052448      0.128638   \n",
       "0.9        0.056969       1.738521     0.067539        1.736631      0.067456   \n",
       "\n",
       "     bayes_true_mean  bayes_true_sd  \n",
       "0.1       310.815833      17.173429  \n",
       "0.2        99.719351       4.684317  \n",
       "0.3        46.827814       2.812289  \n",
       "0.4        27.593595       0.887082  \n",
       "0.5        17.170802       0.556397  \n",
       "0.6        10.874407       0.392665  \n",
       "0.7         6.579686       0.267581  \n",
       "0.8         3.790642       0.093496  \n",
       "0.9         1.591423       0.063267  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(data=np.transpose([mme_means, mme_sds,csc_means, csc_sds, monte_carlo_means, monte_carlo_sds, bayes_mc_means, bayes_mc_sds,bayes_csc_means, bayes_csc_sds, bayes_true_means, bayes_true_sds]), columns = [\"mme_mean\", \"mme_sd\",\"csc_mean\",\"csc_sd\",\"monte_carlo_mean\",\"monte_carlo_sd\",\"bayes_mc_mean\",\"bayes_mc_sd\",\"bayes_csc_mean\",\"bayes_csc_sd\",\"bayes_true_mean\",\"bayes_true_sd\"],index=p_range)\n",
    "display(results_df)\n",
    "results_df.to_csv(folder+\"/degree_error_dataframe.csv\", index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for figure 5 (degree vs estimated degree for small and large p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"STACKEX\"\n",
    "net_name = \"SX\"\n",
    "p_small, p_big = 0.1, 0.9\n",
    "\n",
    "p=0.9\n",
    "\n",
    "G = nx.read_edgelist(folder+\"/REAL\")\n",
    "\n",
    "H = nx.read_edgelist(folder+\"/\"+net_name+str(round(p,1))+\"-1\")\n",
    "kp_seq = [d for (_,d) in nx.degree(H)]\n",
    "kp_full = kp_seq+[0 for _ in range(G.number_of_nodes() - H.number_of_nodes())]\n",
    "\n",
    "## True degree sequence of nodes in G that are in H\n",
    "\n",
    "true_degrees = np.array([nx.degree(G)[n] for n in H.nodes])\n",
    "full_degrees = np.array([nx.degree(G)[n] for n in H.nodes]+[nx.degree(G)[n] for n in G.nodes if n not in H.nodes])\n",
    "\n",
    "mme_seq = deg_MME(kp_full,p,True)\n",
    "\n",
    "csc_seq = degree_cascade(kp_seq,len(full_degrees) - len(true_degrees),p)\n",
    "\n",
    "mc_deg, ssd_error, accepted = monte_carlo_degree(kp_full,p)\n",
    "\n",
    "posterior_mc = bayes_approx(mc_deg,kp_full,p)\n",
    "\n",
    "csc_H = np.array(csc_seq)\n",
    "csc_bayes_degrees = bayes_approx(csc_H,kp_full,p)\n",
    "posterior_true = bayes_approx(full_degrees,kp_full,p)\n",
    "\n",
    "degree_df = pd.DataFrame(data=np.transpose([full_degrees, mme_seq,csc_seq,mc_deg,posterior_mc,csc_bayes_degrees,posterior_true]), columns=[\"true_degrees\",\"mme\", \"cascade\",\"monte_carlo\",\"bayes_mc\",\"bayes_cascade\",\"bayes_true\"])\n",
    "degree_df.to_csv(folder+\"/deg-sequences-\"+str(round(p,1))+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangle Count and Distribution Estimators\n",
    "\n",
    "This section will use the estimators from degree but for the number of triangles per edge and in total.\n",
    "\n",
    "## Basic functions\n",
    "\n",
    "First we define some utility functions for counting number of common neighbours between node pairs and triangles per edge, as well as wedge counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbours(n1, n2, G):\n",
    "    n1neigh = set(G.neighbors(n1))\n",
    "    n2neigh = set(G.neighbors(n2))\n",
    "    return len(n1neigh.intersection(n2neigh)), G.has_edge(n1,n2)\n",
    "\n",
    "def triangles(n1, n2, G):\n",
    "    if not G.has_edge(n1,n2):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(list(nx.common_neighbors(G,n1,n2)))\n",
    "\n",
    "def edge_triangle_count(graph,edge_subset=None):\n",
    "    if edge_subset is not None:\n",
    "        edges = edge_subset\n",
    "    else:\n",
    "        edges = graph.edges()\n",
    "    tc=[]\n",
    "    for u,v in edges:\n",
    "        tc.append(triangles(u,v,graph))\n",
    "    return tc\n",
    "\n",
    "def edge_triangle_dict(G):\n",
    "    tc={}\n",
    "    for u,v in G.edges():\n",
    "        tc[(min(u,v),max(u,v))]=triangles(u,v,G)\n",
    "    return tc\n",
    "\n",
    "\n",
    "def local_wedge_count(G):\n",
    "    nodes = sorted(G.nodes())\n",
    "    ec = []\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i):\n",
    "            cn = common_neighbours(nodes[i],nodes[j],G)\n",
    "            if cn[0]!=0:\n",
    "                ec.append(cn[0])\n",
    "    return ec\n",
    "\n",
    "def wedge_count(G):\n",
    "    return 0.5*sum([d*(d-1) for n,d in G.degree()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MME estimators for triangles and clustering\n",
    "\n",
    "Some functions for MME estimators of clustering coefficient and triangle count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_MME(tri_array, p):\n",
    "    return np.array([round(1.0/p**2*ct) for ct in tri_array])\n",
    "\n",
    "def scaleup_wedge(H,p):\n",
    "    return 1.0/p**2*wedge_count(H)\n",
    "\n",
    "def scaleup_local_wedge(H,p):\n",
    "    return 1.0/p**2*np.array(H)\n",
    "\n",
    "def scaleup_clustering_coeff(H,p):\n",
    "    wc = 1.0/p**2*wedge_count(H)\n",
    "    t_array = tri_MME(edge_triangle_count(H),p)\n",
    "    tc = sum(t_array)\n",
    "    return t_array/wc\n",
    "\n",
    "def tot_tri_estimate(H,p):\n",
    "    tri_array = edge_triangle_count(H)/p**3\n",
    "    return sum(tri_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Mass Functions for triangle count of sampled network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangle_likelihood(t,tc,p):\n",
    "    return comb(t,tc)*np.power((1.0-p*p),t)\n",
    "\n",
    "def triangle_likelihood_normalised(t,tc,p):\n",
    "    return comb(t,tc)*np.power(p,2.0*tc)*np.power(1.0-p*p,t-tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for making a cache of pre-calculated probabilities: like a defaultdict but it has \n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class key_dependent_dict(defaultdict):\n",
    "    def __init__(self,f_of_x):\n",
    "        super().__init__(None) # base class doesn't get a factory\n",
    "        self.f_of_x = f_of_x # save f(x)\n",
    "    def __missing__(self, key): # called when a default needed\n",
    "        ret = self.f_of_x(key) # calculate default value\n",
    "        self[key] = ret # and install it in the dict\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "def tri_bayes_approx_pois(tl_sampled, prob_retain, T_lambda):\n",
    "    m = len(tl_sampled)\n",
    "    # t_max = len(prior)-1\n",
    "\n",
    "    posterior = np.zeros(m, dtype=float)\n",
    "\n",
    "    # ## no need to keep re-evaluating probabilities over and over\n",
    "    # prob_cache = key_dependent_dict(lambda x : binom(x[0],x[1],prob_retain**2))\n",
    "\n",
    "    vectorised_binom_num = lambda tc: np.vectorize(lambda t: t*binom(t,tc,prob_retain**2),excluded=[\"tc\"])\n",
    "    vectorised_binom_den = lambda tc: np.vectorize(lambda t: binom(t,tc,prob_retain**2),excluded=[\"tc\"])\n",
    "\n",
    "    ## no need to keep re-evaluating probabilities over and over\n",
    "    prob_cache_num = key_dependent_dict(lambda tc : poisson(T_lambda).expect(vectorised_binom_num(tc), lb=tc))\n",
    "    prob_cache_den = key_dependent_dict(lambda tc : poisson(T_lambda).expect(vectorised_binom_den(tc), lb=tc))\n",
    "\n",
    "    for edge in range(m):\n",
    "        t_observed = tl_sampled[edge]\n",
    "        # t_range = np.array(range(t_observed, t_max + 1))\n",
    "        \n",
    "        # binom_ev = np.vectorize(lambda t: prob_cache[(t,int(t_observed))],otypes=[float])\n",
    "\n",
    "        # denom = poisson(T_lambda).expect(lambda t: t*binom(t,t_observed,prob_retain**2), lb=t_observed, maxcount=100)\n",
    "        # numer = poisson(T_lambda).expect(lambda t: binom(t,t_observed,prob_retain**2), lb=t_observed, maxcount=100)\n",
    "        \n",
    "        denom = prob_cache_den[t_observed]\n",
    "        numer = prob_cache_num[t_observed]\n",
    "        if (denom > 0):\n",
    "            posterior[edge] = numer/denom\n",
    "        else:\n",
    "            posterior[edge] = 0.0\n",
    "\n",
    "    return np.array(posterior)\n",
    "\n",
    "def poisson_prior(mean, tol):\n",
    "    total = 0.0\n",
    "    ind = 0\n",
    "    prior_arr = []\n",
    "    \n",
    "    while (total < 1.0-tol):\n",
    "        prior_arr.append(poisson.pmf(ind,mean))\n",
    "        total+=prior_arr[-1]\n",
    "        ind+=1\n",
    "    return np.array(prior_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_bayes_approx_true(tl_sampled, prob_retain, dist):\n",
    "    m = len(tl_sampled)\n",
    "    # t_max = len(prior)-1\n",
    "\n",
    "    posterior = np.zeros(m, dtype=float)\n",
    "\n",
    "    # ## no need to keep re-evaluating probabilities over and over\n",
    "    # prob_cache = key_dependent_dict(lambda x : binom(x[0],x[1],prob_retain**2))\n",
    "\n",
    "    vectorised_binom_num = lambda tc: np.vectorize(lambda t: t*binom(t,tc,prob_retain**2),excluded=[\"tc\"])\n",
    "    vectorised_binom_den = lambda tc: np.vectorize(lambda t: binom(t,tc,prob_retain**2),excluded=[\"tc\"])\n",
    "\n",
    "    ## no need to keep re-evaluating probabilities over and over\n",
    "    prob_cache_num = key_dependent_dict(lambda tc : dist.expect(vectorised_binom_num(tc), lb=tc, maxcount=100))\n",
    "    prob_cache_den = key_dependent_dict(lambda tc : dist.expect(vectorised_binom_den(tc), lb=tc, maxcount=100))\n",
    "\n",
    "    for edge in range(m):\n",
    "        t_observed = tl_sampled[edge]\n",
    "        # t_range = np.array(range(t_observed, t_max + 1))\n",
    "        \n",
    "        # binom_ev = np.vectorize(lambda t: prob_cache[(t,int(t_observed))],otypes=[float])\n",
    "\n",
    "        # denom = poisson(T_lambda).expect(lambda t: t*binom(t,t_observed,prob_retain**2), lb=t_observed, maxcount=100)\n",
    "        # numer = poisson(T_lambda).expect(lambda t: binom(t,t_observed,prob_retain**2), lb=t_observed, maxcount=100)\n",
    "        \n",
    "        denom = prob_cache_den[t_observed]\n",
    "        numer = prob_cache_num[t_observed]\n",
    "        if (denom > 0):\n",
    "            posterior[edge] = numer/denom\n",
    "        else:\n",
    "            posterior[edge] = 0.0\n",
    "\n",
    "    return np.array(posterior)\n",
    "\n",
    "def poisson_prior(mean, tol):\n",
    "    total = 0.0\n",
    "    ind = 0\n",
    "    prior_arr = []\n",
    "    \n",
    "    while (total < 1.0-tol):\n",
    "        prior_arr.append(poisson.pmf(ind,mean))\n",
    "        total+=prior_arr[-1]\n",
    "        ind+=1\n",
    "    return np.array(prior_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangle count main experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:3800: RuntimeWarning: expect(): sum did not converge\n",
      "  warnings.warn('expect(): sum did not converge', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1\n",
      "Experiment 2\n",
      "Experiment 3\n",
      "Experiment 4\n",
      "Experiment 5\n",
      "Experiment 6\n",
      "Experiment 7\n",
      "Experiment 8\n",
      "Experiment 9\n"
     ]
    }
   ],
   "source": [
    "# N,M=1000,10000\n",
    "# G = nx.gnm_random_graph(N,M)\n",
    "\n",
    "# G = nx.powerlaw_cluster_graph(10000,5,0.3)\n",
    "\n",
    "folder=\"STACKEX\"\n",
    "name=\"SX\"\n",
    "G = nx.read_edgelist(folder+'/REAL')\n",
    "M = G.number_of_edges()\n",
    "\n",
    "Tl_real_full = edge_triangle_count(G)\n",
    "maxT = max(Tl_real_full)\n",
    "T_real = sum(Tl_real_full)/3.0\n",
    "\n",
    "t_counts = Counter(Tl_real_full)\n",
    "pk = [t_counts[t]/M for t in range(0,maxT + 1)]\n",
    "true_prior = rv_discrete(\"true\",values= (np.array(range(maxT+1)), pk))\n",
    "\n",
    "p_range = np.linspace(0.1,0.9,9)\n",
    "experiments = 10\n",
    "\n",
    "mme_tri_dist_err = np.zeros((experiments,len(p_range)))\n",
    "mme_tri_tot_err = np.zeros((experiments,len(p_range)))\n",
    "\n",
    "bayes_pois_dist_err = np.zeros((experiments,len(p_range)))\n",
    "bayes_pois_tot_err = np.zeros((experiments,len(p_range)))\n",
    "\n",
    "bayes_true_dist_err = np.zeros((experiments,len(p_range)))\n",
    "bayes_true_tot_err = np.zeros((experiments,len(p_range)))\n",
    "\n",
    "def single_run_tri(run_number):\n",
    "    for ind, p in enumerate(p_range):\n",
    "        H = nx.read_edgelist(folder+'/'+name+str(round(p,1))+'-'+str(run_number))\n",
    "\n",
    "        N_prime, M_prime = H.number_of_nodes(), H.number_of_edges()\n",
    "        missing_edges = round(M_prime*(1.0/p - 1.0))\n",
    "\n",
    "        ## Sampled triangle counts and total\n",
    "        tl_sampled = np.array(edge_triangle_count(H, H.edges())+[0 for _ in range(missing_edges)])\n",
    "        T_sampled = sum(tl_sampled)/3.0\n",
    "\n",
    "        ## Real edge triangle count over the subset of H's edges\n",
    "        tl_real = edge_triangle_count(G,H.edges())\n",
    "\n",
    "        ## MME scaleup\n",
    "        mme_tri = tri_MME(tl_sampled,p)\n",
    "        T_mme = T_sampled/p**3\n",
    "\n",
    "        mme_tri_dist_err[run_number, ind] = mse(mme_tri[:M_prime],tl_real[:M_prime])\n",
    "        mme_tri_tot_err[run_number,ind] = (T_mme - T_real)**2\n",
    "\n",
    "        ## Bayes approx Poisson\n",
    "        T_lambda = T_mme*3.0/(M_prime + missing_edges)\n",
    "\n",
    "        bayes_true_tri = tri_bayes_approx_true(tl_sampled[:M_prime], p, true_prior)\n",
    "        bayes_true_dist_err[run_number,ind] = mse(bayes_true_tri[:M_prime],tl_real[:M_prime])\n",
    "\n",
    "        # bayes_pois_tri = tri_bayes_approx(tl_sampled[:M_prime],p,poisson_prior(T_lambda,0.000001))\n",
    "        bayes_pois_tri = tri_bayes_approx_pois(tl_sampled[:M_prime],p,T_lambda)\n",
    "        bayes_pois_dist_err[run_number,ind] = mse(bayes_pois_tri[:M_prime],tl_real[:M_prime])\n",
    "\n",
    "        T_bayes_pois = sum(bayes_pois_tri)/(3.0 * p)\n",
    "        T_bayes_true = sum(bayes_true_tri)/(3.0 * p)\n",
    "        bayes_pois_tot_err[run_number,ind] = (T_bayes_pois - T_real)**2\n",
    "        bayes_true_tot_err[run_number,ind] = (T_bayes_true - T_real)**2\n",
    "\n",
    "        \n",
    "for ex in range(experiments):\n",
    "    print(\"Experiment \"+str(ex))\n",
    "    single_run_tri(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_tri_dist_mean = mme_tri_dist_err.mean(axis=0)\n",
    "mme_tri_tot_mean = mme_tri_tot_err.mean(axis=0)\n",
    "mme_tri_dist_sd = mme_tri_dist_err.std(axis=0)\n",
    "mme_tri_tot_sd = mme_tri_tot_err.std(axis=0)\n",
    "\n",
    "bayes_tri_dist_mean = bayes_pois_dist_err.mean(axis=0)\n",
    "bayes_tri_tot_mean = bayes_pois_tot_err.mean(axis=0)\n",
    "bayes_tri_dist_sd = bayes_pois_dist_err.std(axis=0)\n",
    "bayes_tri_tot_sd = bayes_pois_tot_err.std(axis=0)\n",
    "\n",
    "bayes_tri_true_dist_mean = bayes_true_dist_err.mean(axis=0)\n",
    "bayes_tri_true_tot_mean = bayes_true_tot_err.mean(axis=0)\n",
    "bayes_tri_true_dist_sd = bayes_true_dist_err.std(axis=0)\n",
    "bayes_tri_true_tot_sd = bayes_true_tot_err.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mme_dist_mean</th>\n",
       "      <th>mme_dist_sd</th>\n",
       "      <th>mme_tot_mean</th>\n",
       "      <th>mme_tot_sd</th>\n",
       "      <th>bayes_dist_mean</th>\n",
       "      <th>bayes_dist_sd</th>\n",
       "      <th>bayes_tot_mean</th>\n",
       "      <th>bayes_tot_sd</th>\n",
       "      <th>bayes_dist_true_mean</th>\n",
       "      <th>bayes_dist_true_sd</th>\n",
       "      <th>bayes_tot_true_mean</th>\n",
       "      <th>bayes_tot_true_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>2271.986291</td>\n",
       "      <td>82.261747</td>\n",
       "      <td>2.461447e+09</td>\n",
       "      <td>2.723859e+09</td>\n",
       "      <td>1390.330563</td>\n",
       "      <td>34.044035</td>\n",
       "      <td>2.461447e+09</td>\n",
       "      <td>2.723859e+09</td>\n",
       "      <td>867.319352</td>\n",
       "      <td>16.927738</td>\n",
       "      <td>3.532719e+08</td>\n",
       "      <td>4.231429e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>542.639446</td>\n",
       "      <td>13.886696</td>\n",
       "      <td>1.459633e+09</td>\n",
       "      <td>2.056422e+09</td>\n",
       "      <td>1293.814602</td>\n",
       "      <td>24.982014</td>\n",
       "      <td>1.459408e+09</td>\n",
       "      <td>2.055987e+09</td>\n",
       "      <td>386.703641</td>\n",
       "      <td>7.778796</td>\n",
       "      <td>8.216063e+08</td>\n",
       "      <td>1.195872e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>226.137372</td>\n",
       "      <td>6.698207</td>\n",
       "      <td>1.632687e+09</td>\n",
       "      <td>1.893586e+09</td>\n",
       "      <td>1171.634853</td>\n",
       "      <td>19.253007</td>\n",
       "      <td>1.631733e+09</td>\n",
       "      <td>1.893795e+09</td>\n",
       "      <td>194.577329</td>\n",
       "      <td>3.742175</td>\n",
       "      <td>1.256592e+09</td>\n",
       "      <td>1.459951e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>117.311890</td>\n",
       "      <td>1.421506</td>\n",
       "      <td>3.650151e+08</td>\n",
       "      <td>5.117375e+08</td>\n",
       "      <td>998.064419</td>\n",
       "      <td>8.433404</td>\n",
       "      <td>3.647134e+08</td>\n",
       "      <td>5.115483e+08</td>\n",
       "      <td>108.142787</td>\n",
       "      <td>1.682875</td>\n",
       "      <td>3.144816e+08</td>\n",
       "      <td>4.399140e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>67.034513</td>\n",
       "      <td>1.472438</td>\n",
       "      <td>4.477075e+08</td>\n",
       "      <td>5.761282e+08</td>\n",
       "      <td>798.025024</td>\n",
       "      <td>6.792928</td>\n",
       "      <td>4.479194e+08</td>\n",
       "      <td>5.763653e+08</td>\n",
       "      <td>64.461325</td>\n",
       "      <td>1.831170</td>\n",
       "      <td>4.146825e+08</td>\n",
       "      <td>5.416645e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>39.945234</td>\n",
       "      <td>1.001169</td>\n",
       "      <td>3.059305e+08</td>\n",
       "      <td>2.663762e+08</td>\n",
       "      <td>585.311539</td>\n",
       "      <td>3.636955</td>\n",
       "      <td>3.059370e+08</td>\n",
       "      <td>2.663427e+08</td>\n",
       "      <td>39.000715</td>\n",
       "      <td>0.603344</td>\n",
       "      <td>2.909429e+08</td>\n",
       "      <td>2.553673e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>23.357048</td>\n",
       "      <td>0.539441</td>\n",
       "      <td>1.945954e+08</td>\n",
       "      <td>2.128052e+08</td>\n",
       "      <td>379.979002</td>\n",
       "      <td>5.259731</td>\n",
       "      <td>1.946898e+08</td>\n",
       "      <td>2.146665e+08</td>\n",
       "      <td>22.897836</td>\n",
       "      <td>0.398201</td>\n",
       "      <td>1.889113e+08</td>\n",
       "      <td>2.060667e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>12.780708</td>\n",
       "      <td>0.330915</td>\n",
       "      <td>3.804060e+07</td>\n",
       "      <td>3.647559e+07</td>\n",
       "      <td>210.187778</td>\n",
       "      <td>4.745946</td>\n",
       "      <td>4.379946e+07</td>\n",
       "      <td>4.218354e+07</td>\n",
       "      <td>12.623696</td>\n",
       "      <td>0.318116</td>\n",
       "      <td>3.743353e+07</td>\n",
       "      <td>3.584679e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>5.375992</td>\n",
       "      <td>0.141840</td>\n",
       "      <td>3.502482e+07</td>\n",
       "      <td>3.246054e+07</td>\n",
       "      <td>104.071357</td>\n",
       "      <td>2.512904</td>\n",
       "      <td>5.414607e+07</td>\n",
       "      <td>5.578499e+07</td>\n",
       "      <td>5.259020</td>\n",
       "      <td>0.151625</td>\n",
       "      <td>3.472433e+07</td>\n",
       "      <td>3.218323e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mme_dist_mean  mme_dist_sd  mme_tot_mean    mme_tot_sd  bayes_dist_mean  \\\n",
       "0.1    2271.986291    82.261747  2.461447e+09  2.723859e+09      1390.330563   \n",
       "0.2     542.639446    13.886696  1.459633e+09  2.056422e+09      1293.814602   \n",
       "0.3     226.137372     6.698207  1.632687e+09  1.893586e+09      1171.634853   \n",
       "0.4     117.311890     1.421506  3.650151e+08  5.117375e+08       998.064419   \n",
       "0.5      67.034513     1.472438  4.477075e+08  5.761282e+08       798.025024   \n",
       "0.6      39.945234     1.001169  3.059305e+08  2.663762e+08       585.311539   \n",
       "0.7      23.357048     0.539441  1.945954e+08  2.128052e+08       379.979002   \n",
       "0.8      12.780708     0.330915  3.804060e+07  3.647559e+07       210.187778   \n",
       "0.9       5.375992     0.141840  3.502482e+07  3.246054e+07       104.071357   \n",
       "\n",
       "     bayes_dist_sd  bayes_tot_mean  bayes_tot_sd  bayes_dist_true_mean  \\\n",
       "0.1      34.044035    2.461447e+09  2.723859e+09            867.319352   \n",
       "0.2      24.982014    1.459408e+09  2.055987e+09            386.703641   \n",
       "0.3      19.253007    1.631733e+09  1.893795e+09            194.577329   \n",
       "0.4       8.433404    3.647134e+08  5.115483e+08            108.142787   \n",
       "0.5       6.792928    4.479194e+08  5.763653e+08             64.461325   \n",
       "0.6       3.636955    3.059370e+08  2.663427e+08             39.000715   \n",
       "0.7       5.259731    1.946898e+08  2.146665e+08             22.897836   \n",
       "0.8       4.745946    4.379946e+07  4.218354e+07             12.623696   \n",
       "0.9       2.512904    5.414607e+07  5.578499e+07              5.259020   \n",
       "\n",
       "     bayes_dist_true_sd  bayes_tot_true_mean  bayes_tot_true_sd  \n",
       "0.1           16.927738         3.532719e+08       4.231429e+08  \n",
       "0.2            7.778796         8.216063e+08       1.195872e+09  \n",
       "0.3            3.742175         1.256592e+09       1.459951e+09  \n",
       "0.4            1.682875         3.144816e+08       4.399140e+08  \n",
       "0.5            1.831170         4.146825e+08       5.416645e+08  \n",
       "0.6            0.603344         2.909429e+08       2.553673e+08  \n",
       "0.7            0.398201         1.889113e+08       2.060667e+08  \n",
       "0.8            0.318116         3.743353e+07       3.584679e+07  \n",
       "0.9            0.151625         3.472433e+07       3.218323e+07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(data=np.transpose([mme_tri_dist_mean, mme_tri_dist_sd,mme_tri_tot_mean, mme_tri_tot_sd, \n",
    "bayes_tri_dist_mean, bayes_tri_dist_sd, bayes_tri_tot_mean, bayes_tri_tot_sd, \n",
    "bayes_tri_true_dist_mean, bayes_tri_true_dist_sd, bayes_tri_true_tot_mean, bayes_tri_true_tot_sd]), \n",
    "columns = [\"mme_dist_mean\", \"mme_dist_sd\",\"mme_tot_mean\",\"mme_tot_sd\",\n",
    "\"bayes_dist_mean\",\"bayes_dist_sd\",\"bayes_tot_mean\",\"bayes_tot_sd\",\n",
    "\"bayes_dist_true_mean\",\"bayes_dist_true_sd\",\"bayes_tot_true_mean\",\"bayes_tot_true_sd\"],index=p_range)\n",
    "display(results_df)\n",
    "results_df.to_csv(folder+\"/tri_error_dataframe.csv\", index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real vs estimated triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"STACKEX\"\n",
    "name=\"SX\"\n",
    "G = nx.read_edgelist(folder+'/REAL')\n",
    "M = G.number_of_edges()\n",
    "p=0.9\n",
    "\n",
    "# Real triangle counts\n",
    "Tl_real_full = edge_triangle_count(G)\n",
    "maxT = max(Tl_real_full)\n",
    "T_real = sum(Tl_real_full)/3.0\n",
    "\n",
    "# Turn these real counts into a prior for Bayes\n",
    "t_counts = Counter(Tl_real_full)\n",
    "pk = [t_counts[t]/M for t in range(0,maxT + 1)]\n",
    "true_prior = rv_discrete(\"true\",values= (np.array(range(maxT+1)), pk))\n",
    "\n",
    "H = nx.read_edgelist(folder+'/'+name+str(round(p,1))+'-1')\n",
    "\n",
    "N_prime, M_prime = H.number_of_nodes(), H.number_of_edges()\n",
    "missing_edges = round(M_prime*(1.0/p - 1.0))\n",
    "\n",
    "## Sampled triangle counts and total\n",
    "tl_sampled = np.array(edge_triangle_count(H, H.edges())+[0 for _ in range(missing_edges)])\n",
    "T_sampled = sum(tl_sampled)/3.0\n",
    "\n",
    "## Real edge triangle count over the subset of H's edges\n",
    "tl_real = edge_triangle_count(G,H.edges())\n",
    "\n",
    "## MME scaleup\n",
    "mme_tri = tri_MME(tl_sampled,p)\n",
    "T_mme = T_sampled/p**3\n",
    "\n",
    "## Bayes approx Poisson\n",
    "T_lambda = T_mme*3.0/(M_prime + missing_edges)\n",
    "bayes_pois_tri = tri_bayes_approx_pois(tl_sampled[:M_prime],p,T_lambda)\n",
    "\n",
    "## Bayes true prior\n",
    "bayes_true_tri = tri_bayes_approx_true(tl_sampled[:M_prime], p, true_prior)\n",
    "\n",
    "tri_df = pd.DataFrame(data=np.transpose([tl_real[:M_prime], mme_tri[:M_prime], bayes_pois_tri[:M_prime], bayes_true_tri[:M_prime]]), columns=[\"true\",\"mme\",\"bayes_pois\", \"bayes_true\"])\n",
    "tri_df.to_csv(folder+\"/tri_real_vs_estimated_\"+str(round(p,1))+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
